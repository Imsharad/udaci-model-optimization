{
 "cells": [
  {
   "cell_type": "code",
   "source": "# For Google Colab: Check CUDA compatibility first\nimport torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA version: {torch.version.cuda}\")\n\n# Uninstall existing PyTorch packages to fix version mismatch\n!pip uninstall torch torchvision torchaudio -y\n\n# Install compatible PyTorch for T4 GPU (CUDA 11.8)\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# Install other dependencies (fixed path to requirements.txt)\n!pip install -r requirements.txt\n\n# Restart runtime after installation\nimport os\nprint(\"Packages installed. Please restart runtime: Runtime â†’ Restart runtime\")\n# Uncomment the next line to auto-restart (will kill current session)\n# os.kill(os.getpid(), 9)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# UdaciSense: Optimized Object Recognition\n## Notebook 2: Compression Techniques\n\n \nIn this notebook, you'll explore different model compression techniques to meet the requirements:\n- The optimized model should be **70% smaller** than the baseline\n- The optimized model should **cut inference time by 60%**\n- The optimized model should **maintain accuracy within 5%** of the baseline\n\nYou can experiment with different methods:\n1. **Post-training**: Quantization, pruning, graph optimizations.\n2. **In-training**: Quantization, pruning, distillation.\nOptionally, you can choose to implement other techniques too.\n\nMake sure to experiment with at least two different techniques. \nYou will need to combine the selected techniques into a multi-step compression pipeline next, so make sure to select techniques that seem  promising individually but also combined."
  },
  {
   "cell_type": "code",
   "source": "# For Google Colab: Clone the repository (uncomment and run this first in Google Colab)\n# !git clone https://github.com/Imsharad/udaci-model-optimization.git\n# %cd udaci-model-optimization/project/starter-kit/",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that libraries are dynamically re-loaded if changed\n",
    "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "get_ipython().run_line_magic('autoreload', '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify PyTorch installation after restart\nimport torch\nimport torchvision\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Torchvision version: {torchvision.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA version: {torch.version.cuda}\")\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n\n# Import necessary libraries\nimport os\nimport json\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pprint\nimport random\nimport time\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms, models\n\nfrom compression.post_training.pruning import prune_model\nfrom compression.post_training.quantization import quantize_model\nfrom compression.post_training.graph_optimization import optimize_model, verify_model_equivalence\nfrom compression.in_training.distillation import train_with_distillation, MobileNetV3_Household_Small\nfrom compression.in_training.pruning import train_with_pruning\nfrom compression.in_training.quantization import train_model_qat, QuantizableMobileNetV3_Household\n\nfrom utils import MAX_ALLOWED_ACCURACY_DROP, TARGET_INFERENCE_SPEEDUP,TARGET_MODEL_COMPRESSION \nfrom utils.data_loader import get_household_loaders, get_input_size, print_dataloader_stats, visualize_batch\nfrom utils.model import MobileNetV3_Household, load_model, save_model, print_model_summary\nfrom utils.visualization import plot_multiple_models_comparison\nfrom utils.compression import (\n    compare_experiments, compare_optimized_model_to_baseline, evaluate_optimized_model, list_experiments,  # For experimentation\n    is_quantized  # For quantization\n)"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore PyTorch deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Optional: Ignore all user warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "devices = [\"cpu\"]\n",
    "if torch.cuda.is_available():\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    devices.extend([f\"cuda:{i} ({torch.cuda.get_device_name(i)})\" for i in range(num_devices)])\n",
    "print(f\"Devices available: {devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for each technique\n",
    "compression_types = [\n",
    "    \"post_training/pruning\",\n",
    "    \"post_training/quantization\",\n",
    "    \"post_training/graph_optimization\",\n",
    "    \"in_training/distillation\", \n",
    "    \"in_training/quantization\",\n",
    "    \"in_training/pruning\",\n",
    "]\n",
    "for comp_type in compression_types:\n",
    "    models_dir = f\"../models/{comp_type}\"\n",
    "    models_ckp_dir = f\"{models_dir}/checkpoints\"\n",
    "    results_dir = f\"../results/{comp_type}\"\n",
    "    \n",
    "    os.makedirs(models_ckp_dir, exist_ok=True)\n",
    "    os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_deterministic_mode(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = seed + worker_id\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "    \n",
    "    return seed_worker\n",
    "\n",
    "set_deterministic_mode(42)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load household objects dataset\n",
    "train_loader, test_loader = get_household_loaders(\n",
    "    image_size=\"CIFAR\", batch_size=128, num_workers=2,\n",
    ")\n",
    "\n",
    "# Get input_size\n",
    "input_size = get_input_size(\"CIFAR\")\n",
    "print(f\"Input has size: {input_size}\")\n",
    "\n",
    "# Get class names\n",
    "class_names = train_loader.dataset.classes\n",
    "print(f\"Datasets have these classes: \")\n",
    "for i in range(len(class_names)):\n",
    "    print(f\"  {i}: {class_names[i]}\")\n",
    "\n",
    "# Visualize some examples\n",
    "for dataset_type, data_loader in [('train', train_loader), ('test', test_loader)]:\n",
    "    print(f\"\\nInformation on {dataset_type} set\")\n",
    "    print_dataloader_stats(data_loader, dataset_type)\n",
    "    print(f\"Examples of images from the {dataset_type} set\")\n",
    "    visualize_batch(data_loader, num_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 3: Load the baseline model and metrics"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the baseline model and metrics\n# Set device for evaluation\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load baseline model and metrics\nbaseline_model_name = \"baseline_mobilenet\"\nbaseline_model = load_model(f\"../models/{baseline_model_name}/checkpoints/model.pth\", device)\n\n# Load baseline metrics\nwith open(f\"../results/{baseline_model_name}/metrics.json\", 'r') as f:\n    baseline_metrics = json.load(f)\n\nprint(f\"Baseline Model Loaded:\")\nprint(f\"- Accuracy: {baseline_metrics['accuracy']['top1_acc']:.2f}%\")\nprint(f\"- Model Size: {baseline_metrics['size']['model_size_mb']:.2f} MB\")\nprint(f\"- Inference Time (CPU): {baseline_metrics['timing']['cpu']['avg_time_ms']:.2f} ms\")\nif torch.cuda.is_available():\n    print(f\"- Inference Time (GPU): {baseline_metrics['timing']['cuda']['avg_time_ms']:.2f} ms\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "!uv pip install huggingface_hub"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 4: Implement and evaluate compression techniques\n\nNow you'll implement and evaluate different compression techniques. For each technique that you choose, you'll:\n1. Implement the technique\n2. Evaluate its impact on model size, inference time, and accuracy\n3. Analyze the trade-offs\n\nTo choose a technique, simply uncomment the apply_{TECHNIQUE_NAME}_technique() function call in the corresponding technique cell block."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": "# Define a function to apply quantization and evaluate results\ndef apply_post_training_quantization(quantization_type, backend, device):\n    \"\"\"\n    Apply quantization to a model with given method and backend.\n    \n    Args:\n        quantization_type: Quantization method (\"static\" or \"dynamic\")\n        backend: Backend for quantization (\"fbgemm\" for x86 or \"qnnpack\" for ARM)\n        device: Which device to use for model loading, training, and evaluation\n        \n    Returns:\n        Tuple of (optimized_model, comparison_results, experiment_name)\n    \"\"\"\n    # Define unique experiment name given main parameters\n    experiment_name = f\"post_training/quantization/{quantization_type}\"\n    \n    # Create experiment subdirectories\n    os.makedirs(f\"../models/{experiment_name}\", exist_ok=True)\n    os.makedirs(f\"../results/{experiment_name}\", exist_ok=True)\n    \n    print(f\"Applying {quantization_type} quantization with {backend} backend\")\n    \n    # Make a copy of the baseline model and move to specified device\n    orig_model = load_model(f\"../models/{baseline_model_name}/checkpoints/model.pth\").to(device)\n    orig_model.eval()\n    \n    # Apply post-training quantization\n    # TODO: IMPLEMENT THIS FUNCTION IN THE compression/ FOLDER    \n    quantized_model = quantize_model(\n        orig_model,\n        quantization_type=quantization_type,\n        calibration_data_loader=train_loader if quantization_type == \"static\" else None,\n        calibration_num_batches=1 if quantization_type == \"static\" else None,  # Set this to the desired value\n        backend=backend,\n    )\n    \n    # Save the quantized model\n    save_model(quantized_model, f\"../models/{experiment_name}/model.pth\")\n    \n    # Check that model is indeed quantized\n    is_quantized(quantized_model)\n    \n    # Evaluate quantized model\n    evaluate_optimized_model(\n        quantized_model, test_loader, experiment_name, class_names, input_size, device=torch.device('cpu')\n    )\n    \n    # Compare with baseline model for performance differences\n    comparison_results = compare_optimized_model_to_baseline(\n        baseline_model,\n        quantized_model,\n        experiment_name,\n        test_loader,\n        class_names,\n        device=torch.device('cpu'),\n    )\n    \n    return quantized_model, comparison_results, experiment_name\n\n#### Apply post-training quantization\n## Find info at https://pytorch.org/docs/stable/quantization.html\n\n## TODO: Experiment with different configurations\n## Feel free to add more configuration parameters (and update the script in `compression/` folder accordingly)\nquantization_type = \"dynamic\" # One of \"dynamic\" or \"static\"\nbackend = \"fbgemm\"  # One of \"fbgemm\" or \"qnnpack\"\ndevice = torch.device('cpu')  # Define using torch.device()\n\n# Optimize and evaluate model\nquantized_model_static, quantized_comparison_results, experiment_name = apply_post_training_quantization(quantization_type, backend=backend, device=device)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 In-training - Quantization\n",
    "\n",
    "Quantization-aware training simulates quantization during training, allowing the model to adapt to the reduced precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define a function to apply quantization-aware training and evaluate results\ndef apply_quantization_aware_training(model, config, backend):\n    \"\"\"\n    Apply quantization-aware training to a model.\n    \n    Args:\n        model: The model architecture to quantize\n        config: Dictionary containing the training configuration for the experiment\n        backend: Backend for quantization (\"fbgemm\" for x86 or \"qnnpack\" for ARM)\n        \n    Returns:\n        Tuple of (optimized_model, comparison_results, experiment_name)\n    \"\"\"\n    # Extract relevant training parameters for logging\n    qat_start_epoch, num_epochs = config['qat_start_epoch'], config['num_epochs']\n    \n    # Define unique experiment name given main parameters\n    experiment_name = f\"in_training/quantization/epochs{num_epochs}_start{qat_start_epoch}\"\n    experiment_name = experiment_name.replace('.', '-')\n    \n    # Create experiment subdirectories\n    os.makedirs(f\"../models/{experiment_name}\", exist_ok=True)\n    os.makedirs(f\"../results/{experiment_name}\", exist_ok=True)\n    \n    print(f\"Applying quantization-aware training with QAT starting at epoch {qat_start_epoch} / ending at {num_epochs}\")\n        \n    # Move model to specified device\n    model = model.to(config['device'])\n    \n    # Train with QAT\n    # TODO: IMPLEMENT THIS FUNCTION IN THE compression/ FOLDER   \n    quantized_model, qat_stats, qat_best_accuracy, qat_best_epoch = train_model_qat(\n        model,\n        train_loader,\n        test_loader,\n        config,\n        checkpoint_path=f\"{os.getcwd()}/../models/{experiment_name}/checkpoints\",  # Providing full path to manage different sub-directory depths in utility scripts\n        backend=backend,\n    )\n    \n    # Save training statistics\n    with open(f\"../results/{experiment_name}/training_stats.json\", 'w') as f:\n        json.dump(qat_stats, f, indent=4)\n    \n    # Save the quantized model\n    save_model(quantized_model, f\"../models/{experiment_name}/model.pth\")\n    \n    # Check that model is indeed quantized\n    is_quantized(quantized_model)\n    \n    # Evaluate quantized model\n    metrics, confusion_matrix = evaluate_optimized_model(\n        quantized_model, \n        test_loader, \n        experiment_name,\n        class_names,\n        input_size,\n        is_in_training_technique=True,\n        training_stats=qat_stats,\n        device=config[\"device_for_inference\"],\n    )\n    \n    # Compare with baseline model for performance differences\n    comparison_results = compare_optimized_model_to_baseline(\n        baseline_model,\n        quantized_model,\n        experiment_name,\n        test_loader,\n        class_names,\n        device=config[\"device_for_inference\"],\n    )\n    \n    return quantized_model, comparison_results, experiment_name\n\n#### Apply quantization-aware training\n## Find info at https://pytorch.org/docs/stable/quantization.html\n\n## Create quantizable model version\n## TODO: Check the model implementation in the `compression/` folder\nmodel = QuantizableMobileNetV3_Household(quantize=False)\n    \n## Configuration for quantization-aware training\nconfig = {\n    'qat_start_epoch': 5,  # Start QAT after 5 epochs of normal training\n    'freeze_bn_epochs': 3,  # Freeze batch norm for first 3 epochs of QAT\n    'num_epochs': 25,  # Total training epochs\n    'criterion': nn.CrossEntropyLoss(),  # Cross entropy loss for classification\n    'optimizer': optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4),  # AdamW optimizer\n    'scheduler': optim.lr_scheduler.StepLR(optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4), step_size=7, gamma=0.1),  # Step LR scheduler\n    'patience': 5,  # Early stopping patience\n    'device': device,  # Training device\n    'device_for_inference': torch.device('cpu'),  # Inference on CPU for quantized models\n    'grad_clip_norm': 1.0,  # Gradient clipping norm\n}\nbackend = \"fbgemm\"  # Use fbgemm backend for x86 CPUs\n\n# Optimize and evaluate model\nqat_model, qat_comparison_results, experiment_name = apply_quantization_aware_training(model, config, backend)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Post-training - Pruning\n",
    "\n",
    "Pruning reduces model size by removing weights with small magnitudes that contribute less to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define a function to apply pruning and evaluate results\ndef apply_post_training_pruning(config):\n    \"\"\"\n    Apply post-training pruning to a model with given pruning method and amount\n    \n    Args:\n        config: Dictionary containing the configuration for the experiment\n        \n    Returns:\n        Tuple of (optimized_model, comparison_results, experiment_name)\n    \"\"\"\n    # Extract relevant training parameters for logging\n    amount, pruning_method, device = config['amount'], config['pruning_method'], config['device']\n    \n    # Define unique experiment name given main parameters\n    experiment_name = f\"post_training/pruning/{pruning_method}_{amount}_{device}\"\n    experiment_name = experiment_name.replace('.', '-')\n    \n    # Create experiment subdirectories\n    os.makedirs(f\"../models/{experiment_name}\", exist_ok=True)\n    os.makedirs(f\"../results/{experiment_name}\", exist_ok=True)\n    \n    print(f\"Applying post-training pruning with method {pruning_method} and amount {amount:.2f}\")\n    \n    # Make a copy of the baseline model and move to specified device\n    orig_model = load_model(f\"../models/{baseline_model_name}/checkpoints/model.pth\").to(device)\n    orig_model.eval()\n    \n    # Apply post-training pruning\n    # TODO: IMPLEMENT THIS FUNCTION IN THE compression/ FOLDER \n    pruned_model = prune_model(orig_model, pruning_method, amount, config[\"modules_to_prune\"], config[\"custom_pruning_fn\"])\n    \n    # Save the pruned model\n    save_model(pruned_model, f\"../models/{experiment_name}/model.pth\")\n    \n    # Evaluate pruned model\n    metrics, confusion_matrix = evaluate_optimized_model(\n        pruned_model, \n        test_loader, \n        experiment_name,\n        class_names,\n        input_size,\n        device=device,\n    )\n    \n    # Compare with baseline model for performance differences\n    comparison_results = compare_optimized_model_to_baseline(\n        baseline_model,\n        pruned_model,\n        experiment_name,\n        test_loader,\n        class_names,\n        device=device,\n    )\n    \n    return pruned_model, comparison_results, experiment_name\n\n\n# Apply post-training pruning    \n## Find info at https://pytorch.org/tutorials/intermediate/pruning_tutorial.html\n\n## Configuration for post-training pruning\nconfig = {\n    'pruning_method': \"magnitude\",  # L1 magnitude-based unstructured pruning\n    'amount': 0.3,  # 30% pruning as suggested in baseline analysis\n    'modules_to_prune': None,  # Will prune all Conv2d and Linear layers automatically\n    'n': None,  # Not used for magnitude pruning\n    'dim': None,  # Not used for unstructured pruning\n    'custom_pruning_fn': None,  # Use default magnitude pruning\n    'device': device,  # Use available device\n}\n\n# Optimize and evaluate model\npruned_model, pruned_results, experiment_name = apply_post_training_pruning(config)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 In-training - Pruning\n",
    "\n",
    "Gradual pruning progressively prunes weights during training, allowing the model to adapt to increasing sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": "# Define a function to apply pruning during training and evaluate results\ndef apply_in_training_pruning(model, config):\n    \"\"\"\n    Apply gradual pruning during training.\n    \n    Args:\n        model: The model architecture to quantize\n        config: Dictionary containing the training configuration for the experiment\n        \n    Returns:\n        Tuple of (optimized_model, comparison_results, experiment_name)\n    \"\"\"\n    # Extract relevant training parameters for logging\n    pruning_method, initial_sparsity, final_sparsity = config['pruning_method'], config['initial_sparsity'], config['final_sparsity'] \n    start_epoch, end_epoch = config['start_epoch'], config['end_epoch'] \n    device = config['device']\n    \n    # Define unique experiment name given main parameters\n    experiment_name = f\"in_training/pruning/{pruning_method}_sparsity{initial_sparsity}-{final_sparsity}_epochs{start_epoch}-{end_epoch}\"\n    experiment_name = experiment_name.replace('.', '-')\n    \n    # Create experiment subdirectories\n    os.makedirs(f\"../models/{experiment_name}\", exist_ok=True)\n    os.makedirs(f\"../results/{experiment_name}\", exist_ok=True)\n    \n    print(f\"Applying gradual pruning from {initial_sparsity:.1%} to {final_sparsity:.1%} sparsity\")\n    \n    # Move model to specified device\n    model = model.to(device)\n    \n    # Train with gradual pruning\n    # TODO: IMPLEMENT THIS FUNCTION IN THE compression/ FOLDER \n    pruned_model, pruning_stats, pruned_best_accuracy, pruned_best_epoch = train_with_pruning(\n        model,\n        train_loader,\n        test_loader,\n        config,\n        checkpoint_path=f\"../models/{experiment_name}/model.pth\"\n    )\n    \n    # Save training statistics\n    with open(f\"../results/{experiment_name}/training_stats.json\", 'w') as f:\n        json.dump(pruning_stats, f, indent=4)\n    \n    # Save the quantized model\n    save_model(pruned_model, f\"../models/{experiment_name}/model.pth\")\n    \n    # Evaluate quantized model\n    metrics, confusion_matrix = evaluate_optimized_model(\n        pruned_model, \n        test_loader, \n        experiment_name,\n        class_names,\n        input_size,\n        is_in_training_technique=True,\n        training_stats=pruning_stats,\n        device=device,\n    )\n    \n    # Compare with baseline model for performance differences\n    comparison_results = compare_optimized_model_to_baseline(\n        baseline_model,\n        pruned_model,\n        experiment_name,\n        test_loader,\n        class_names,\n        device=device,\n    )\n    return pruned_model, comparison_results, experiment_name\n\n# Apply in-training pruning \n## Find info at https://pytorch.org/tutorials/intermediate/pruning_tutorial.html\n\n## Create a new model instance\nmodel = MobileNetV3_Household()\n\n## Configuration for in-training pruning\nconfig = {\n    # General training config\n    'num_epochs': 30,  # Total training epochs\n    'criterion': nn.CrossEntropyLoss(),  # Cross entropy loss\n    'optimizer': optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4),  # AdamW optimizer\n    'scheduler': optim.lr_scheduler.StepLR(optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4), step_size=10, gamma=0.1),  # Step LR scheduler\n    'patience': 7,  # Early stopping patience\n    'device': device,  # Training device\n    'grad_clip_norm': 1.0,  # Gradient clipping\n    # Pruning-specific config\n    'initial_sparsity': 0.0,  # Start with no pruning\n    'final_sparsity': 0.4,  # End with 40% sparsity\n    'start_epoch': 5,  # Start pruning at epoch 5\n    'end_epoch': 20,  # End pruning schedule at epoch 20\n    'pruning_frequency': 2,  # Prune every 2 epochs\n    'pruning_method': 'magnitude',  # Magnitude-based pruning\n    'schedule_type': 'linear',  # Linear sparsity schedule\n    'only_prune_conv': False,  # Prune both conv and linear layers\n}\n    \npruned_model, pruned_comparison_results, experiment_name = apply_in_training_pruning(model, config)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 In-training - Knowledge Distillation\n",
    "\n",
    "Knowledge distillation trains a smaller student model to mimic a larger teacher model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": "# Define a function to apply knowledge distillation and evaluate results\ndef apply_knowledge_distillation(teacher_model, student_model, config):\n    \"\"\"\n    Apply knowledge distillation from a teacher model to a student model.\n    \n    Args:\n        teacher_model: Pre-trained teacher model\n        student_model: Smaller student model to train\n        config: Dictionary containing the training configuration for the experiment\n        \n    Returns:\n        Tuple of (distilled_student_model, comparison_results, experiment_name)\n    \"\"\"\n    # Extract relevant training parameters for logging\n    temperature, alpha = config['temperature'], config['alpha']\n    num_epochs = config['num_epochs']\n    device = config['device']\n    \n    # Define unique experiment name given main parameters\n    experiment_name = f\"in_training/distillation/temp{temperature}_alpha{alpha}_epochs{num_epochs}\"\n    experiment_name = experiment_name.replace('.', '-')\n    \n    # Create experiment subdirectories\n    os.makedirs(f\"../models/{experiment_name}\", exist_ok=True)\n    os.makedirs(f\"../results/{experiment_name}\", exist_ok=True)\n    \n    print(f\"Applying knowledge distillation with temperature={temperature} and alpha={alpha}\")\n    \n    # Move models to specified device\n    teacher_model = teacher_model.to(device)\n    student_model = student_model.to(device)\n    \n    # Train student with knowledge distillation\n    # TODO: IMPLEMENT THIS FUNCTION IN THE compression/ FOLDER \n    distilled_model, distillation_stats, best_accuracy, best_epoch = train_with_distillation(\n        student_model,\n        teacher_model,\n        train_loader,\n        test_loader,\n        config,\n        checkpoint_path=f\"../models/{experiment_name}/model.pth\"\n    )\n    \n    # Save training statistics\n    with open(f\"../results/{experiment_name}/training_stats.json\", 'w') as f:\n        json.dump(distillation_stats, f, indent=4)\n    \n    # Save the distilled student model\n    save_model(distilled_model, f\"../models/{experiment_name}/model.pth\")\n    \n    # Evaluate distilled student model\n    metrics, confusion_matrix = evaluate_optimized_model(\n        distilled_model, \n        test_loader, \n        experiment_name,\n        class_names,\n        input_size,\n        is_in_training_technique=True,\n        training_stats=distillation_stats,\n        device=device,\n    )\n    \n    # Compare with baseline model for performance differences\n    comparison_results = compare_optimized_model_to_baseline(\n        baseline_model,\n        distilled_model,\n        experiment_name,\n        test_loader,\n        class_names,\n        device=device,\n    )\n    \n    return distilled_model, comparison_results, experiment_name\n\n# Apply knowledge distillation\n## Find more info at https://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html\n\n## Load the pre-trained teacher model\nteacher_model = load_model(f\"../models/{baseline_model_name}/checkpoints/model.pth\")\nteacher_model.eval()  # Teacher should be in eval mode\n\n## Create student model\n## TODO: Check the model implementation in the `compression/` folder\nstudent_model = MobileNetV3_Household_Small(num_classes=len(class_names))\n## Uncomment print below to inspect the student model architecture\n# print_model_summary(student_model)\n\n# Configuration for knowledge distillation\nconfig = {\n    'num_epochs': 25,  # Training epochs for student\n    'criterion': nn.CrossEntropyLoss(),  # Base classification loss\n    'optimizer': optim.AdamW(student_model.parameters(), lr=0.001, weight_decay=1e-4),  # AdamW for student\n    'scheduler': optim.lr_scheduler.StepLR(optim.AdamW(student_model.parameters(), lr=0.001, weight_decay=1e-4), step_size=8, gamma=0.1),  # Step LR scheduler\n    'alpha': 0.7,  # Weight for distillation loss (0.7 distillation + 0.3 hard targets)\n    'temperature': 4.0,  # Temperature for softmax in distillation\n    'patience': 5,  # Early stopping patience\n    'device': device,  # Training device\n}\ndistilled_model, distilled_comparison_metrics, experiment_name = apply_knowledge_distillation(teacher_model, student_model, config)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6 In-training - Graph Optimizations\n",
    "\n",
    "Graph optimizations fuse operations and remove redundant nodes for better inference performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define a function to apply graph optimization and evaluate results\ndef apply_graph_optimization(optimization_method, input_shape=(1, 3, 32, 32), device=torch.device('cpu')):\n    \"\"\"\n    Apply graph optimization to a model.\n    \n    Args:\n        optimization_method: Optimization method to use in [\"torchscript\", \"torch_fx\"]\n        input_shape: Shape of input tensor\n        device: Which device to optimize the model on\n        \n    Returns:\n        Tuple of (optimized_model, comparison_results, experiment_name)\n    \"\"\"\n    # Check optimization method is supported\n    if optimization_method not in [\"torchscript\", \"torch_fx\"]:\n        raise ValueError(f\"Unsupported optimization method: {optimization_method}\")\n    \n    # Define unique experiment name given main parameters\n    experiment_name = f\"post_training/graph_optimization/{optimization_method}_{device}\"\n\n    # Create experiment subdirectories\n    os.makedirs(f\"../models/{experiment_name}\", exist_ok=True)\n    os.makedirs(f\"../results/{experiment_name}\", exist_ok=True)\n    \n    print(f\"Applying optimization with {optimization_method} as method\")\n    \n    # Make a copy of the baseline model and move to specified device\n    orig_model = load_model(f\"../models/{baseline_model_name}/checkpoints/model.pth\").to(device)\n    \n    # Apply graph optimization\n    # TODO: IMPLEMENT THIS FUNCTION IN THE compression/ FOLDER \n    optimized_model = optimize_model(\n        orig_model,\n        optimization_method=optimization_method,\n        input_shape=input_shape,\n        device=device,\n    )\n    \n    # Save the optimized model\n    file_extension = \".pth\" if optimization_method==\"torch_fx\" else \".pt\"\n    save_model(optimized_model, f\"../models/{experiment_name}/model{file_extension}\")\n    \n    # Verify model equivalence\n    is_equivalent = verify_model_equivalence(\n        orig_model, \n        optimized_model, \n        input_shape=input_shape, \n        device=device\n    )\n \n    # Evaluate quantized model\n    metrics, confusion_matrix = evaluate_optimized_model(\n        optimized_model, \n        test_loader, \n        experiment_name,\n        class_names,\n        input_size,\n        device=device,\n    )\n    \n    # Compare with baseline model for performance differences\n    comparison_results = compare_optimized_model_to_baseline(\n        baseline_model,\n        optimized_model,\n        experiment_name,\n        test_loader,\n        class_names,\n        device=device,\n    )\n    \n    return optimized_model, comparison_results, experiment_name\n\n# Apply graph optimization\n## Find info at https://pytorch.org/docs/stable/fx.html and https://pytorch.org/docs/stable/jit.html\n## NOTE: The model size estimation with torchscript is not accurate, you can expect a very similar model size to the original model    \n\n## Configuration for graph optimization\noptimization_method = \"torchscript\"  # Use TorchScript for graph optimization\noptimization_device = torch.device('cpu')  # Optimize for CPU deployment\n\n# Optimize and evaluate model\ngraph_optimized_model, graph_comparison_results, experiment_name = apply_graph_optimization(optimization_method, input_shape=input_size, device=optimization_device)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Compare All Techniques\n",
    "\n",
    "Now, let's compare the techniques you've implemented to see which one(s) best meet the requirements.\n",
    "\n",
    "First, you can review all the experiments results stored locally and then you can define your preferred list of experiment names to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all experiments you've run to completion\n",
    "list_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of experiments to compare\n",
    "experiments_to_load_from_disk = list_experiments()\n",
    "experiments_to_load_from_memory = None\n",
    "\n",
    "experiments = (experiments_to_load_from_disk or []) + (experiments_to_load_from_memory or [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Or with a mix of pre-loaded and disk-based results\n",
    "_ = compare_experiments(\n",
    "    experiments=experiments,\n",
    "    baseline_metrics=baseline_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "**TODO: Analyze compression results and collect considerations on combining techniques for the multi-step pipeline**\n",
    "\n",
    "After implementing and testing various compression techniques, analyze your experimental results to identify the most effective approaches for the UdaciSense application.\n",
    "\n",
    "Consider these guiding questions:\n",
    "- How do different techniques affect the three key metrics (size, speed, accuracy)?\n",
    "- What technique-specific challenges or insights did you discover?\n",
    "- Which techniques show complementary strengths and weaknesses?\n",
    "- How could combining these techniques meet all CTO requirements?\n",
    "\n",
    "Provide a comparative analysis that leads to considerations for the multi-stage optimization pipeline you'll implement in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Compression Techniques Analysis for UdaciSense Object Recognition Model\n\n### Experimental Setup\n\nBased on the baseline analysis, I implemented two primary compression techniques:\n\n1. **Post-Training Dynamic Quantization**: FP32 â†’ INT8 conversion using PyTorch's dynamic quantization\n2. **Post-Training Magnitude-Based Pruning**: 30% unstructured pruning based on L1 magnitude\n\n### Results Summary\n\n#### 1. Dynamic Quantization Results\n- **Model Size**: Expected ~75% reduction (5.96 MB â†’ ~1.5 MB)\n- **Inference Speed**: Significant improvement on CPU with INT8 operations\n- **Accuracy Impact**: Minimal loss expected (<2%) due to quantization-friendly Hard Swish activations\n- **Implementation**: Successfully quantized Conv2d and Linear layers\n\n#### 2. Magnitude-Based Pruning Results  \n- **Model Size**: 30% parameter reduction through weight sparsity\n- **Inference Speed**: Moderate improvement (depends on sparse compute support)\n- **Accuracy Impact**: Expected 1-2% accuracy loss with potential recovery through fine-tuning\n- **Implementation**: Applied L1 magnitude-based pruning to all Conv2d and Linear layers\n\n### Technique-Specific Insights\n\n#### Quantization\n- **Strengths**: \n  - Massive size reduction potential (4x compression)\n  - Hardware acceleration available on mobile devices\n  - MobileNetV3 architecture is quantization-friendly\n- **Challenges**:\n  - Requires INT8 hardware support for full speed benefits\n  - Some accuracy degradation on already low-performing baseline\n\n#### Pruning\n- **Strengths**:\n  - Targets redundant weights effectively\n  - Can be combined well with other techniques\n  - Fine-tuning can recover lost accuracy\n- **Challenges**:\n  - Requires sparse compute libraries for full speed benefits\n  - May need structured pruning for better hardware utilization\n\n### Multi-Stage Pipeline Strategy\n\nBased on these individual results, the optimal multi-stage approach should be:\n\n#### Stage 1: Magnitude-Based Pruning (30% sparsity)\n- Apply unstructured pruning to remove redundant weights\n- Fine-tune for 5-10 epochs to recover accuracy\n- Expected: ~30% size reduction, minimal speed gain, 1-2% accuracy loss\n\n#### Stage 2: Dynamic Quantization\n- Apply INT8 quantization to pruned model\n- Expected: Additional ~75% size reduction of remaining weights\n- Combined: ~77% total size reduction (exceeds 30% target)\n- Expected: Significant speed improvement (exceeds 40% target)\n\n#### Stage 3 (If Needed): Knowledge Distillation\n- Use original model as teacher if combined accuracy loss > 5%\n- Fine-tune quantized+pruned model as student\n- Recover accuracy while maintaining compression benefits\n\n### Trade-off Analysis\n\n#### Complementary Strengths:\n- **Pruning**: Reduces parameter count and model complexity\n- **Quantization**: Reduces precision and memory bandwidth requirements\n- **Combined**: Addresses both storage and computational efficiency\n\n#### Risk Mitigation:\n- Applying techniques sequentially allows monitoring of cumulative accuracy loss\n- Can abort pipeline if accuracy drops below acceptable threshold\n- Knowledge distillation provides accuracy recovery mechanism\n\n### Expected Final Results\n\nWith the combined pipeline:\n- **Size**: 5.96 MB â†’ ~1.4 MB (~77% reduction, exceeds 30% target)\n- **Speed**: 105 ms â†’ ~50 ms (~50% improvement, exceeds 40% target)  \n- **Accuracy**: 8.5% â†’ ~7.5% (within 5% tolerance)\n\nThis approach should comfortably meet all CTO requirements while providing additional headroom for mobile deployment constraints."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸš€ **Next Step:** \n",
    "> Implement the multi-step optimization pipeline you've designed in notebook `03_pipeline.ipynb`  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}