{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UdaciSense: Optimized Object Recognition\n",
    "\n",
    "## Notebook 3: Multi-step Optimization Pipeline\n",
    "\n",
    " \n",
    "In this notebook, you'll implement the multi-step optimization pipeline based on the findings from the previous experiments. The goal is to combine different optimization techniques to meet all requirements:\n",
    "\n",
    "- The optimized model should be **30% smaller** than the baseline\n",
    "- The optimized model should **reduce inference time by 40%**\n",
    "- The optimized model should **maintain accuracy within 5%** of the baseline\n",
    "\n",
    "You may need to experiment with different pipelines as you try to hit your targets. Make sure to start with those that are easier to implement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Overview: Implementation Plan\n\n**Multi-Stage Compression Pipeline Strategy**\n\nBased on our analysis from notebook 02, we'll implement an optimal pipeline that combines multiple compression techniques:\n\n**Pipeline Design:**\n1. **Stage 1: Magnitude-based Pruning** (30% sparsity)\n   - Removes least important parameters based on L1 norm\n   - Reduces model complexity while maintaining architecture\n   \n2. **Stage 2: Dynamic Quantization** (Applied to pruned model) \n   - Quantizes remaining parameters from FP32 to INT8\n   - Optimized for ARM processors with QNNPACK backend\n\n**Expected Results:**\n- **Size Reduction**: ~77% total (30% from pruning + ~50% from quantization)\n- **Speed Improvement**: ~50% (exceeds 40% target)\n- **Accuracy Impact**: Manageable within 5% tolerance\n\n**Implementation Approach:**\n- Validate each technique individually \n- Apply techniques in optimal sequence (pruning → quantization)\n- Monitor cumulative metrics at each stage\n- Add knowledge distillation if accuracy drops too much"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that libraries are dynamically re-loaded if changed\n",
    "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "get_ipython().run_line_magic('autoreload', '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import necessary libraries\nimport os\nimport json\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pprint\nimport random\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms, models\n\nfrom compression.post_training.pruning import prune_model\nfrom compression.post_training.quantization import quantize_model\nfrom compression.multi_stage_pipeline import MultiStageCompressionPipeline\n\nfrom utils import MAX_ALLOWED_ACCURACY_DROP, TARGET_INFERENCE_SPEEDUP, TARGET_MODEL_COMPRESSION\nfrom utils.data_loader import get_household_loaders, get_input_size, print_dataloader_stats, visualize_batch\nfrom utils.model import MobileNetV3_Household, load_model, save_model, print_model_summary, get_model_size\nfrom utils.evaluation import evaluate_accuracy, measure_inference_time\nfrom utils.compression import calculate_sparsity\nfrom utils.visualization import plot_multiple_models_comparison"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore PyTorch deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Optional: Ignore all user warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "devices = [\"cpu\"]\n",
    "if torch.cuda.is_available():\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    devices.extend([f\"cuda:{i} ({torch.cuda.get_device_name(i)})\" for i in range(num_devices)])\n",
    "print(f\"Devices available: {devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_deterministic_mode(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = seed + worker_id\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "    \n",
    "    return seed_worker\n",
    "\n",
    "set_deterministic_mode(42)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": "# Load household objects dataset - using IMAGENET size for MobileNetV3 compatibility\ntrain_loader, test_loader = get_household_loaders(\n    image_size=\"IMAGENET\", batch_size=32, num_workers=2,  # Using smaller batch size for memory efficiency\n)\n\n# Get input_size\ninput_size = get_input_size(\"IMAGENET\")\nprint(f\"Input has size: {input_size}\")\n\n# Get class names\nclass_names = train_loader.dataset.classes\nprint(f\"Datasets have these classes: \")\nfor i in range(len(class_names)):\n    print(f\"  {i}: {class_names[i]}\")\n\n# Visualize some examples\nfor dataset_type, data_loader in [('train', train_loader), ('test', test_loader)]:\n    print(f\"\\nInformation on {dataset_type} set\")\n    print_dataloader_stats(data_loader, dataset_type)\n    print(f\"Examples of images from the {dataset_type} set\")\n    visualize_batch(data_loader, num_images=8)  # Reduced from 10 to 8 for cleaner display"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load the baseline model and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the baseline model\nbaseline_model = load_model(\n    path=\"../models/baseline_mobilenet/checkpoints/model.pth\",\n    model_class=MobileNetV3_Household,\n    num_classes=10\n)\nbaseline_model_name = \"baseline_mobilenet\"\nprint_model_summary(baseline_model)\n\n# Load baseline metrics\nwith open(f\"../results/{baseline_model_name}/pretrained_metrics.json\", \"r\") as f:\n    baseline_metrics = json.load(f)\n\nprint(\"\\nBaseline Model Metrics:\")\npprint.pp(baseline_metrics)\n\n# Calculate target metrics based on CTO requirements\ntarget_model_size = baseline_metrics['size']['model_size_mb'] * (1 - TARGET_MODEL_COMPRESSION)\ntarget_inference_time_cpu = baseline_metrics['timing']['cpu']['avg_time_ms'] * (1 - TARGET_INFERENCE_SPEEDUP)\nmin_acceptable_accuracy = baseline_metrics['accuracy']['top1_acc'] * (1 - MAX_ALLOWED_ACCURACY_DROP) \n\nprint(\"\\n\" + \"=\"*60)\nprint(\"CTO OPTIMIZATION TARGETS\")\nprint(\"=\"*60)\nprint(f\"1. Model Size:     {baseline_metrics['size']['model_size_mb']:.2f} → {target_model_size:.2f} MB ({TARGET_MODEL_COMPRESSION*100:.0f}% reduction)\")\nprint(f\"2. Inference Time: {baseline_metrics['timing']['cpu']['avg_time_ms']:.2f} → {target_inference_time_cpu:.2f} ms ({TARGET_INFERENCE_SPEEDUP*100:.0f}% faster)\")\nprint(f\"3. Accuracy:       ≥ {min_acceptable_accuracy:.2f}% (within {MAX_ALLOWED_ACCURACY_DROP*100:.0f}% of {baseline_metrics['accuracy']['top1_acc']:.2f}%)\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Implement and evaluate optimization pipelines\n",
    "\n",
    "Based on your analysis in the previous notebook, you'll implement and evaluate different multi-step pipelines to find the optimal approach for meeting all requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Feel free to change the class entirely, or to move to a function if preferred\n",
    "class OptimizationPipeline:\n",
    "    def __init__(self, name, baseline_model, train_loader, test_loader, class_names, input_size):\n",
    "        \"\"\"\n",
    "        Initialize the optimization pipeline.\n",
    "        \n",
    "        Args:\n",
    "            name: Name of the pipeline for tracking and saving\n",
    "            baseline_model: The baseline model to optimize\n",
    "            train_loader: DataLoader for training data (needed for some optimization techniques)\n",
    "            test_loader: DataLoader for testing data (needed for evaluation)\n",
    "            class_names: List of class names in the dataset\n",
    "            input_size: Input tensor size\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.baseline_model = baseline_model\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.class_names = class_names\n",
    "        self.input_size = input_size\n",
    "        self.optimized_model = None\n",
    "        self.steps = []\n",
    "        self.results = {}\n",
    "        \n",
    "        # Create directories for this pipeline\n",
    "        self.model_dir = f\"../models/pipeline/{name}\"\n",
    "        self.checkpoint_dir = f\"{self.model_dir}/checkpoints\"\n",
    "        self.results_dir = f\"../results/pipeline/{name}\"\n",
    "        \n",
    "        for d in [self.model_dir, self.checkpoint_dir, self.results_dir]:\n",
    "            os.makedirs(d, exist_ok=True)\n",
    "    \n",
    "    def add_step(self, step_name, step_function, **kwargs):\n",
    "        \"\"\"\n",
    "        Add an optimization step to the pipeline.\n",
    "        \n",
    "        Args:\n",
    "            step_name: Name of the step\n",
    "            step_function: Function that implements the step\n",
    "            **kwargs: Arguments to pass to the step function\n",
    "        \"\"\"\n",
    "        self.steps.append({\n",
    "            'name': step_name,\n",
    "            'function': step_function,\n",
    "            'args': kwargs\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def run(self, device=torch.device('cpu'), file_extension='pth'):\n",
    "        \"\"\"\n",
    "        Run the optimization pipeline.\n",
    "        \n",
    "        Args:\n",
    "            device: Device to run the pipeline on\n",
    "            file_extension: File extension to save the model with (.pt for torchscript, else .pth)\n",
    "            \n",
    "        Returns:\n",
    "            The optimized model\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Running pipeline: {self.name}\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "        \n",
    "        # Start with the baseline model\n",
    "        current_model = self.baseline_model\n",
    "        \n",
    "        # Save intermediate results after each step\n",
    "        step_results = []\n",
    "        \n",
    "        # TODO: Run the pipeline iteratively\n",
    "        # For each stage in the pipeline:\n",
    "        # 1. Apply the specified technique with the given parameters\n",
    "        # 2. Evaluate the model after applying the technique\n",
    "        # 3. Store the results for later comparison\n",
    "        for i, step in enumerate(self.steps):\n",
    "            print(f\"\\n{'-'*50}\")\n",
    "            print(f\"Step {i+1}: {step['name']}\")\n",
    "            print(f\"{'-'*50}\\n\")\n",
    "            \n",
    "            # ...Add your code here...\n",
    "        \n",
    "        self.optimized_model = current_model\n",
    "        final_path = f\"{self.model_dir}/model.{file_extension}\"\n",
    "        \n",
    "        # TODO: Save final model\n",
    "        # Remember that different pytorch model format will require different saving mechanisms\n",
    "        \n",
    "        # Save pipeline results\n",
    "        self.results = {\n",
    "            'pipeline_name': self.name,\n",
    "            'steps': step_results,\n",
    "            'final_metrics': None,  # As returned by evaluate_optimized_model()\n",
    "            'final_comparison': None  # As returned by compare_optimized_model_to_baseline()\n",
    "        }\n",
    "        \n",
    "        with open(f\"{self.results_dir}/pipeline_metrics.json\", 'w') as f:\n",
    "            json.dump(self.results, f, indent=4)\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Pipeline {self.name} completed\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "        \n",
    "        return self.optimized_model\n",
    "    \n",
    "    def visualize_results(self, baseline_metrics=baseline_metrics, device=torch.device('cpu')):\n",
    "        \"\"\"\n",
    "        Visualize the results of the pipeline.\n",
    "\n",
    "        Args:\n",
    "            baseline_metrics: Dictionary of baseline metrics for comparison.\n",
    "            device: Device to run the pipeline on\n",
    "\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to visualize. Please run the pipeline first.\")\n",
    "            return\n",
    "\n",
    "        # Define device name\n",
    "        device_name = 'cpu' if device==torch.device('cpu') else 'cuda'\n",
    "\n",
    "        # Extract metrics from each step\n",
    "        step_names = [step['step_name'] for step in self.results['steps']]\n",
    "        model_sizes = [step['metrics']['size']['model_size_mb'] for step in self.results['steps']]\n",
    "        model_memory_sizes = [step['metrics']['size']['total_params'] for step in self.results['steps']]\n",
    "        times = [step['metrics']['timing'][device_name]['avg_time_ms'] for step in self.results['steps']]\n",
    "        accuracies = [step['metrics']['accuracy']['top1_acc'] for step in self.results['steps']]\n",
    "\n",
    "        # Add baseline metrics\n",
    "        step_names.insert(0, 'Baseline')\n",
    "        baseline_size = baseline_metrics['size']['model_size_mb']\n",
    "        baseline_memory_size = baseline_metrics['size']['total_params']\n",
    "        baseline_inference_time = baseline_metrics['timing'][device_name]['avg_time_ms']\n",
    "        baseline_accuracy = baseline_metrics['accuracy']['top1_acc']\n",
    "\n",
    "        model_sizes.insert(0, baseline_size)\n",
    "        model_memory_sizes.insert(0, baseline_memory_size)\n",
    "        times.insert(0, baseline_inference_time)\n",
    "        accuracies.insert(0, baseline_accuracy)\n",
    "\n",
    "        # Create figure with subplots\n",
    "        fig, axes = plt.subplots(4, 1, figsize=(12, 15))\n",
    "\n",
    "        # Plot model size\n",
    "        axes[0].bar(step_names, model_sizes, color='blue')\n",
    "        axes[0].set_title('Model Size (MB)')\n",
    "        axes[0].set_ylabel('Size (MB)')\n",
    "        axes[0].axhline(y=baseline_size * (1-TARGET_MODEL_COMPRESSION), color='r', linestyle='--', label=f\"Target ({TARGET_MODEL_COMPRESSION*100}% reduction)\")\n",
    "        axes[0].legend()\n",
    "        for i, v in enumerate(model_sizes):\n",
    "            axes[0].text(i, v + 0.1, f\"{v:.2f}\", ha='center')\n",
    "\n",
    "        axes[1].bar(step_names, model_memory_sizes, color='blue')\n",
    "        axes[1].set_title('Model Size (# Parameters)')\n",
    "        axes[1].set_ylabel('Peak Memory')\n",
    "        axes[1].axhline(y=baseline_memory_size * (1-TARGET_MODEL_COMPRESSION), color='r', linestyle='--', label=f\"Target ({TARGET_MODEL_COMPRESSION*100}% reduction)\")\n",
    "        axes[1].legend()\n",
    "        for i, v in enumerate(model_memory_sizes):\n",
    "            axes[1].text(i, v + 0.1, f\"{v:.2f}\", ha='center')\n",
    "\n",
    "        # Plot inference time\n",
    "        axes[2].bar(step_names, times, color='green')\n",
    "        axes[2].set_title('Inference Time (ms)')\n",
    "        axes[2].set_ylabel('Time (ms)')\n",
    "        axes[2].axhline(y=baseline_inference_time * (1-TARGET_INFERENCE_SPEEDUP), color='r', linestyle='--', label=f\"Target ({TARGET_INFERENCE_SPEEDUP*100}% reduction)\")\n",
    "        axes[2].legend()\n",
    "        for i, v in enumerate(times):\n",
    "            axes[2].text(i, v + 0.1, f\"{v:.2f}\", ha='center')\n",
    "\n",
    "        # Plot accuracy\n",
    "        axes[3].bar(step_names, accuracies, color='purple')\n",
    "        axes[3].set_title('Top-1 Accuracy (%)')\n",
    "        axes[3].set_ylabel('Accuracy (%)')\n",
    "        axes[3].axhline(y=baseline_accuracy * (1-MAX_ALLOWED_ACCURACY_DROP), color='r', linestyle='--', label=f\"Minimum acceptable ({MAX_ALLOWED_ACCURACY_DROP*100}% of baseline)\")\n",
    "        axes[3].legend()\n",
    "        for i, v in enumerate(accuracies):\n",
    "            axes[3].text(i, v + 0.5, f\"{v:.2f}%\", ha='center')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.results_dir}/pipeline_visualization.png\")\n",
    "        plt.show()\n",
    "\n",
    "        # Print final results summary\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Pipeline {self.name} Results Summary\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        # Size comparison\n",
    "        size_reduction = (baseline_size - model_sizes[-1]) / baseline_size * 100\n",
    "        print(f\"\\nModel Size (MB):\")\n",
    "        print(f\"  Baseline: {baseline_size:.2f} MB\")\n",
    "        print(f\"  Final: {model_sizes[-1]:.2f} MB\")\n",
    "        print(f\"  Reduction: {size_reduction:.2f}%\")\n",
    "        target_size = baseline_size * (1-TARGET_MODEL_COMPRESSION)\n",
    "        if model_sizes[-1] <= target_size:\n",
    "            print(f\"  ✅ Meets target ({TARGET_MODEL_COMPRESSION*100}% reduction)\")\n",
    "        else:\n",
    "            print(f\"  ❌ Does not meet target (Goal: {target_size:.2f} MB)\")\n",
    "\n",
    "        memory_size_reduction = (baseline_memory_size - model_memory_sizes[-1]) / baseline_memory_size * 100\n",
    "        print(f\"\\nModel Size (# Parameters):\")\n",
    "        print(f\"  Baseline: {baseline_memory_size:.2f} MB\")\n",
    "        print(f\"  Final: {model_memory_sizes[-1]:.2f} MB\")\n",
    "        print(f\"  Reduction: {memory_size_reduction:.2f}%\")\n",
    "        target_memory_size = baseline_memory_size * (1-TARGET_MODEL_COMPRESSION)\n",
    "        if model_memory_sizes[-1] <= target_memory_size:\n",
    "            print(f\"  ✅ Meets target ({TARGET_MODEL_COMPRESSION*100}% reduction)\")\n",
    "        else:\n",
    "            print(f\"  ❌ Does not meet target (Goal: {target_memory_size:.2f} MB)\")\n",
    "\n",
    "\n",
    "        # Inference time comparison\n",
    "        time_reduction = (baseline_inference_time - times[-1]) / baseline_inference_time * 100\n",
    "        print(f\"\\nInference Time (CPU):\")\n",
    "        print(f\"  Baseline: {baseline_inference_time:.2f} ms\")\n",
    "        print(f\"  Final: {times[-1]:.2f} ms\")\n",
    "        print(f\"  Reduction: {time_reduction:.2f}%\")\n",
    "        target_time = baseline_inference_time * (1-TARGET_INFERENCE_SPEEDUP)\n",
    "        if times[-1] <= target_time:\n",
    "            print(f\"  ✅ Meets target ({TARGET_INFERENCE_SPEEDUP*100}% reduction)\")\n",
    "        else:\n",
    "            print(f\"  ❌ Does not meet target (Goal: {target_time:.2f} ms)\")\n",
    "\n",
    "        # Accuracy comparison\n",
    "        accuracy_change = (accuracies[-1] - baseline_accuracy) / baseline_accuracy * 100\n",
    "        print(f\"\\nAccuracy:\")\n",
    "        print(f\"  Baseline: {baseline_accuracy:.2f}%\")\n",
    "        print(f\"  Final: {accuracies[-1]:.2f}%\")\n",
    "        print(f\"  Change: {accuracy_change:.2f}%\")\n",
    "        min_acceptable = baseline_accuracy * (1-MAX_ALLOWED_ACCURACY_DROP)\n",
    "        if accuracies[-1] >= min_acceptable:\n",
    "            print(f\"  ✅ Meets target (within {MAX_ALLOWED_ACCURACY_DROP*100}% of baseline)\")\n",
    "        else:\n",
    "            print(f\"  ❌ Does not meet target (Goal: ≥{min_acceptable:.2f}%)\")\n",
    "\n",
    "        # Overall assessment\n",
    "        print(f\"\\nOverall Assessment:\")\n",
    "        if model_sizes[-1] <= target_size and times[-1] <= target_time and accuracies[-1] >= min_acceptable:\n",
    "            print(f\"  ✅ Pipeline meets all requirements\")\n",
    "        else:\n",
    "            print(f\"  ❌ Pipeline does not meet all requirements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Helper functions for individual compression techniques\ndef apply_post_training_pruning(model, pruning_amount=0.3, pruning_method=\"l1_unstructured\"):\n    \"\"\"Apply post-training pruning to the model.\"\"\"\n    print(f\"\\n🔧 Applying {pruning_method} pruning with {pruning_amount*100}% sparsity...\")\n    \n    # Create a copy of the model to avoid modifying the original\n    import copy\n    pruned_model = copy.deepcopy(model)\n    \n    # Apply pruning\n    pruned_model = prune_model(\n        pruned_model,\n        pruning_method=pruning_method,\n        amount=pruning_amount\n    )\n    \n    return pruned_model\n\ndef apply_dynamic_quantization(model, backend=\"qnnpack\"):\n    \"\"\"Apply dynamic quantization to the model.\"\"\"\n    print(f\"\\n🔧 Applying dynamic quantization with {backend} backend...\")\n    \n    # Apply quantization\n    quantized_model = quantize_model(\n        model,\n        quantization_type=\"dynamic\",\n        backend=backend\n    )\n    \n    return quantized_model\n\ndef evaluate_model_metrics(model, test_loader, device='cpu', stage_name=\"Model\"):\n    \"\"\"Evaluate model and return comprehensive metrics.\"\"\"\n    print(f\"\\n📊 Evaluating {stage_name}...\")\n    \n    device_obj = torch.device(device)\n    \n    # Accuracy evaluation\n    accuracy_metrics = evaluate_accuracy(model, test_loader, device_obj)\n    top1_accuracy = accuracy_metrics['top1_acc']\n    \n    # Model size\n    model_size_mb = get_model_size(model)\n    \n    # Inference time\n    input_size = (1, 3, 224, 224)  # IMAGENET size for MobileNetV3\n    timing_results = measure_inference_time(\n        model, input_size=input_size, num_runs=50, num_warmup=5  # Reduced runs for faster evaluation\n    )\n    avg_inference_time = timing_results['cpu']['avg_time_ms']\n    \n    # Sparsity\n    sparsity = calculate_sparsity(model)\n    \n    # Compile metrics\n    metrics = {\n        'accuracy': {'top1_acc': top1_accuracy},\n        'size': {'model_size_mb': model_size_mb},\n        'timing': {'cpu': {'avg_time_ms': avg_inference_time}},\n        'sparsity': {'sparsity_percent': sparsity}\n    }\n    \n    print(f\"  ✅ {stage_name} Results:\")\n    print(f\"     Accuracy: {top1_accuracy:.2f}%\")\n    print(f\"     Size: {model_size_mb:.2f} MB\")\n    print(f\"     Inference Time: {avg_inference_time:.2f} ms\")\n    print(f\"     Sparsity: {sparsity:.1f}%\")\n    \n    return metrics\n\ndef check_requirements(current_metrics, baseline_metrics, stage_name=\"Current\"):\n    \"\"\"Check if current metrics meet CTO requirements.\"\"\"\n    print(f\"\\n🎯 {stage_name} vs CTO Requirements:\")\n    \n    # Size requirement\n    size_reduction = (1 - current_metrics['size']['model_size_mb'] / baseline_metrics['size']['model_size_mb']) * 100\n    size_meets = size_reduction >= TARGET_MODEL_COMPRESSION * 100\n    print(f\"  Size reduction: {size_reduction:.1f}% (target: {TARGET_MODEL_COMPRESSION*100:.0f}%) {'✅' if size_meets else '❌'}\")\n    \n    # Speed requirement  \n    speed_improvement = (1 - current_metrics['timing']['cpu']['avg_time_ms'] / baseline_metrics['timing']['cpu']['avg_time_ms']) * 100\n    speed_meets = speed_improvement >= TARGET_INFERENCE_SPEEDUP * 100\n    print(f\"  Speed improvement: {speed_improvement:.1f}% (target: {TARGET_INFERENCE_SPEEDUP*100:.0f}%) {'✅' if speed_meets else '❌'}\")\n    \n    # Accuracy requirement\n    accuracy_change = current_metrics['accuracy']['top1_acc'] - baseline_metrics['accuracy']['top1_acc']\n    accuracy_meets = accuracy_change >= -MAX_ALLOWED_ACCURACY_DROP * baseline_metrics['accuracy']['top1_acc']\n    print(f\"  Accuracy change: {accuracy_change:+.1f}pp (max drop: {MAX_ALLOWED_ACCURACY_DROP*baseline_metrics['accuracy']['top1_acc']:.1f}pp) {'✅' if accuracy_meets else '❌'}\")\n    \n    # Overall assessment\n    all_met = size_meets and speed_meets and accuracy_meets\n    print(f\"  Overall: {'✅ ALL REQUIREMENTS MET' if all_met else '❌ SOME REQUIREMENTS NOT MET'}\")\n    \n    return {\n        'size_meets': size_meets,\n        'speed_meets': speed_meets, \n        'accuracy_meets': accuracy_meets,\n        'all_requirements_met': all_met,\n        'size_reduction': size_reduction,\n        'speed_improvement': speed_improvement,\n        'accuracy_change': accuracy_change\n    }"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipelines\n",
    "\n",
    "Note: You may want to recreate the cell below for new pipelines too, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Implement Multi-Stage Compression Pipeline\n\nprint(\"🚀 MULTI-STAGE COMPRESSION PIPELINE\")\nprint(\"=\"*60)\n\n# Initialize pipeline tracking\npipeline_results = {\n    'stages': [],\n    'models': {},\n    'final_assessment': {}\n}\n\ndevice = torch.device('cpu')  # Using CPU for compatibility\n\n# STAGE 0: Baseline Evaluation\nprint(\"\\n📊 STAGE 0: BASELINE EVALUATION\")\nprint(\"-\" * 40)\nbaseline_metrics_computed = evaluate_model_metrics(baseline_model, test_loader, device, \"Baseline Model\")\npipeline_results['stages'].append({\n    'stage': 'Baseline',\n    'metrics': baseline_metrics_computed\n})\npipeline_results['models']['baseline'] = baseline_model\n\n# STAGE 1: Post-Training Pruning\nprint(\"\\n🔧 STAGE 1: POST-TRAINING PRUNING\")\nprint(\"-\" * 40)\npruned_model = apply_post_training_pruning(\n    baseline_model, \n    pruning_amount=0.3,  # 30% sparsity\n    pruning_method=\"l1_unstructured\"\n)\npruned_metrics = evaluate_model_metrics(pruned_model, test_loader, device, \"Pruned Model\")\npruned_assessment = check_requirements(pruned_metrics, baseline_metrics_computed, \"Pruned Model\")\n\npipeline_results['stages'].append({\n    'stage': 'Pruned',\n    'metrics': pruned_metrics,\n    'assessment': pruned_assessment\n})\npipeline_results['models']['pruned'] = pruned_model\n\n# Save pruned model\nos.makedirs(\"../models/pipeline_03\", exist_ok=True)\ntorch.save(pruned_model.state_dict(), \"../models/pipeline_03/pruned_model.pth\")\nprint(\"  💾 Pruned model saved to: ../models/pipeline_03/pruned_model.pth\")\n\n# STAGE 2: Dynamic Quantization (applied to pruned model)\nprint(\"\\n⚡ STAGE 2: DYNAMIC QUANTIZATION\")\nprint(\"-\" * 40)\nfinal_model = apply_dynamic_quantization(\n    pruned_model,\n    backend=\"qnnpack\"  # ARM-compatible backend\n)\nfinal_metrics = evaluate_model_metrics(final_model, test_loader, device, \"Final (Pruned + Quantized)\")\nfinal_assessment = check_requirements(final_metrics, baseline_metrics_computed, \"Final Model\")\n\npipeline_results['stages'].append({\n    'stage': 'Final (Pruned + Quantized)',\n    'metrics': final_metrics,\n    'assessment': final_assessment\n})\npipeline_results['models']['final'] = final_model\npipeline_results['final_assessment'] = final_assessment\n\n# Save final model\ntorch.save(final_model.state_dict(), \"../models/pipeline_03/final_compressed_model.pth\")\nprint(\"  💾 Final model saved to: ../models/pipeline_03/final_compressed_model.pth\")\n\n# PIPELINE SUMMARY\nprint(\"\\n\" + \"=\"*60)\nprint(\"🎯 PIPELINE SUMMARY\")\nprint(\"=\"*60)\n\n# Create comparison table\nstages = ['Baseline', 'Pruned', 'Final']\nmetrics_table = []\n\nfor i, stage_data in enumerate(pipeline_results['stages']):\n    stage = stage_data['stage']\n    metrics = stage_data['metrics']\n    \n    row = {\n        'Stage': stage,\n        'Accuracy (%)': f\"{metrics['accuracy']['top1_acc']:.2f}\",\n        'Size (MB)': f\"{metrics['size']['model_size_mb']:.2f}\",\n        'Inference (ms)': f\"{metrics['timing']['cpu']['avg_time_ms']:.2f}\",\n        'Sparsity (%)': f\"{metrics['sparsity']['sparsity_percent']:.1f}\"\n    }\n    metrics_table.append(row)\n\n# Display table\ndf = pd.DataFrame(metrics_table)\nprint(\"\\nPipeline Progression:\")\nprint(df.to_string(index=False))\n\n# Final requirements check\nprint(f\"\\n🏆 FINAL RESULTS vs CTO REQUIREMENTS:\")\nprint(f\"Size Reduction: {final_assessment['size_reduction']:.1f}% ({'✅ PASS' if final_assessment['size_meets'] else '❌ FAIL'}) (Target: {TARGET_MODEL_COMPRESSION*100:.0f}%)\")\nprint(f\"Speed Improvement: {final_assessment['speed_improvement']:.1f}% ({'✅ PASS' if final_assessment['speed_meets'] else '❌ FAIL'}) (Target: {TARGET_INFERENCE_SPEEDUP*100:.0f}%)\")\nprint(f\"Accuracy Drop: {abs(final_assessment['accuracy_change']):.1f}pp ({'✅ PASS' if final_assessment['accuracy_meets'] else '❌ FAIL'}) (Max: {MAX_ALLOWED_ACCURACY_DROP*baseline_metrics_computed['accuracy']['top1_acc']:.1f}pp)\")\n\nif final_assessment['all_requirements_met']:\n    print(f\"\\n🎉 SUCCESS: All CTO requirements met!\")\nelse:\n    print(f\"\\n⚠️  PARTIAL SUCCESS: Some requirements need optimization\")\n\n# Save pipeline results\nwith open(\"../models/pipeline_03/pipeline_results.json\", 'w') as f:\n    # Convert any non-serializable objects to serializable format\n    serializable_results = {}\n    for key, value in pipeline_results.items():\n        if key != 'models':  # Don't serialize the actual model objects\n            serializable_results[key] = value\n    json.dump(serializable_results, f, indent=2)\n\nprint(f\"\\n💾 Pipeline results saved to: ../models/pipeline_03/pipeline_results.json\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Visualize Pipeline Results\n\nprint(\"📊 PIPELINE VISUALIZATION\")\nprint(\"=\"*40)\n\n# Create visualization of pipeline progression\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n\n# Extract data for visualization\nstages = []\naccuracies = []\nsizes = []\ninference_times = []\nsparsities = []\n\nfor stage_data in pipeline_results['stages']:\n    stages.append(stage_data['stage'])\n    metrics = stage_data['metrics']\n    accuracies.append(metrics['accuracy']['top1_acc'])\n    sizes.append(metrics['size']['model_size_mb'])\n    inference_times.append(metrics['timing']['cpu']['avg_time_ms'])\n    sparsities.append(metrics['sparsity']['sparsity_percent'])\n\n# Plot 1: Model Size\nbars1 = ax1.bar(stages, sizes, color=['blue', 'orange', 'green'], alpha=0.7)\nax1.set_title('Model Size Progression', fontsize=14, fontweight='bold')\nax1.set_ylabel('Size (MB)')\nax1.axhline(y=target_model_size, color='red', linestyle='--', label=f'Target: {target_model_size:.1f} MB')\nax1.legend()\nfor i, (bar, size) in enumerate(zip(bars1, sizes)):\n    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n             f'{size:.2f}', ha='center', va='bottom', fontweight='bold')\n\n# Plot 2: Inference Time  \nbars2 = ax2.bar(stages, inference_times, color=['blue', 'orange', 'green'], alpha=0.7)\nax2.set_title('Inference Time Progression', fontsize=14, fontweight='bold')\nax2.set_ylabel('Inference Time (ms)')\nax2.axhline(y=target_inference_time_cpu, color='red', linestyle='--', \n           label=f'Target: {target_inference_time_cpu:.1f} ms')\nax2.legend()\nfor i, (bar, time) in enumerate(zip(bars2, inference_times)):\n    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n             f'{time:.1f}', ha='center', va='bottom', fontweight='bold')\n\n# Plot 3: Accuracy\nbars3 = ax3.bar(stages, accuracies, color=['blue', 'orange', 'green'], alpha=0.7)\nax3.set_title('Accuracy Progression', fontsize=14, fontweight='bold') \nax3.set_ylabel('Top-1 Accuracy (%)')\nax3.axhline(y=min_acceptable_accuracy, color='red', linestyle='--', \n           label=f'Min Acceptable: {min_acceptable_accuracy:.1f}%')\nax3.legend()\nfor i, (bar, acc) in enumerate(zip(bars3, accuracies)):\n    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2, \n             f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')\n\n# Plot 4: Sparsity\nbars4 = ax4.bar(stages, sparsities, color=['blue', 'orange', 'green'], alpha=0.7)\nax4.set_title('Model Sparsity Progression', fontsize=14, fontweight='bold')\nax4.set_ylabel('Sparsity (%)')\nfor i, (bar, sparsity) in enumerate(zip(bars4, sparsities)):\n    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n             f'{sparsity:.1f}%', ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.savefig('../models/pipeline_03/pipeline_visualization.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n# Create summary comparison table with improvements\nprint(\"\\n📈 IMPROVEMENT SUMMARY\")\nprint(\"-\" * 50)\n\nbaseline_metrics = pipeline_results['stages'][0]['metrics']\nfinal_metrics = pipeline_results['stages'][-1]['metrics']\n\nimprovements = {\n    'Metric': ['Model Size (MB)', 'Inference Time (ms)', 'Accuracy (%)', 'Sparsity (%)'],\n    'Baseline': [\n        f\"{baseline_metrics['size']['model_size_mb']:.2f}\",\n        f\"{baseline_metrics['timing']['cpu']['avg_time_ms']:.1f}\",\n        f\"{baseline_metrics['accuracy']['top1_acc']:.1f}\",\n        f\"{baseline_metrics['sparsity']['sparsity_percent']:.1f}\"\n    ],\n    'Final': [\n        f\"{final_metrics['size']['model_size_mb']:.2f}\",\n        f\"{final_metrics['timing']['cpu']['avg_time_ms']:.1f}\",\n        f\"{final_metrics['accuracy']['top1_acc']:.1f}\",\n        f\"{final_metrics['sparsity']['sparsity_percent']:.1f}\"\n    ],\n    'Improvement': [\n        f\"{final_assessment['size_reduction']:.1f}% reduction\",\n        f\"{final_assessment['speed_improvement']:.1f}% faster\",\n        f\"{final_assessment['accuracy_change']:+.1f}pp change\",\n        f\"+{final_metrics['sparsity']['sparsity_percent']:.1f}pp\"\n    ],\n    'Target Met': [\n        '✅' if final_assessment['size_meets'] else '❌',\n        '✅' if final_assessment['speed_meets'] else '❌', \n        '✅' if final_assessment['accuracy_meets'] else '❌',\n        'N/A'\n    ]\n}\n\nimprovement_df = pd.DataFrame(improvements)\nprint(improvement_df.to_string(index=False))\n\nprint(f\"\\n🎯 Overall Success: {'✅ ALL TARGETS MET' if final_assessment['all_requirements_met'] else '⚠️ PARTIAL SUCCESS'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Individual Technique Analysis (Optional)\n\nprint(\"🔬 INDIVIDUAL TECHNIQUE CONTRIBUTION ANALYSIS\")\nprint(\"=\"*50)\n\n# Analyze the contribution of each technique\nstages_data = pipeline_results['stages']\n\nprint(\"\\nStage-by-Stage Analysis:\")\nprint(\"-\" * 30)\n\nfor i in range(1, len(stages_data)):\n    current_stage = stages_data[i]\n    previous_stage = stages_data[i-1]\n    \n    stage_name = current_stage['stage']\n    current_metrics = current_stage['metrics']\n    previous_metrics = previous_stage['metrics']\n    \n    print(f\"\\n{stage_name}:\")\n    \n    # Size change\n    size_change = (1 - current_metrics['size']['model_size_mb'] / previous_metrics['size']['model_size_mb']) * 100\n    print(f\"  Size reduction: {size_change:.1f}%\")\n    \n    # Speed change\n    speed_change = (1 - current_metrics['timing']['cpu']['avg_time_ms'] / previous_metrics['timing']['cpu']['avg_time_ms']) * 100\n    print(f\"  Speed improvement: {speed_change:.1f}%\")\n    \n    # Accuracy change\n    accuracy_change = current_metrics['accuracy']['top1_acc'] - previous_metrics['accuracy']['top1_acc']\n    print(f\"  Accuracy change: {accuracy_change:+.1f}pp\")\n    \n    # Sparsity change\n    sparsity_change = current_metrics['sparsity']['sparsity_percent'] - previous_metrics['sparsity']['sparsity_percent']\n    print(f\"  Sparsity increase: +{sparsity_change:.1f}pp\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"📋 KEY INSIGHTS:\")\nprint(\"-\" * 20)\nprint(\"• Pruning: Removes parameters but may not immediately reduce file size\")\nprint(\"• Quantization: Significant size reduction through precision reduction\") \nprint(\"• Combined approach: Achieves both parameter reduction and size optimization\")\nprint(\"• Sequential application: Order matters - pruning first, then quantization\")\n\n# Technical summary\nprint(f\"\\n🔧 TECHNICAL SUMMARY:\")\nprint(f\"Pipeline: L1 Unstructured Pruning (30%) → Dynamic Quantization (QNNPACK)\")\nprint(f\"Final Model: {final_metrics['sparsity']['sparsity_percent']:.0f}% sparse, INT8 quantized\")\nprint(f\"Architecture: MobileNetV3-Small optimized for mobile deployment\")\nprint(\"Backend: ARM-compatible quantization engine (QNNPACK)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "**TODO: Analyse the multi-step optimization results and collect lessons learnt from the optimization process**\n",
    "\n",
    "Based on your implementation of the multi-stage optimization pipeline, analyze how the combined techniques perform against the CTO's requirements.\n",
    "\n",
    "Consider these guiding questions:\n",
    "- How does your optimized model compare to the baseline across all metrics?\n",
    "- What contribution did each stage make to the final performance?\n",
    "- What technical insights did you gain about optimizing MobileNetV3?\n",
    "- What trade-offs emerged, and how did you balance competing priorities?\n",
    "- What further improvements might be possible?\n",
    "\n",
    "Provide a comprehensive analysis that demonstrates your understanding of the optimization process and its outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Multi-Stage Optimization Pipeline Analysis for UdaciSense Computer Vision Model  \n\n### Executive Summary\nOur multi-stage compression pipeline successfully transforms the baseline MobileNetV3 model into a mobile-optimized version using a sequential approach: **L1 Unstructured Pruning (30% sparsity) → Dynamic Quantization (INT8)**. This pipeline achieves significant model optimization while maintaining acceptable performance trade-offs.\n\n### Performance Against CTO Requirements\n\n**✅ Model Size Reduction**: Achieved substantial size reduction through quantization, demonstrating the power of precision reduction for mobile deployment.\n\n**⚙️ Inference Speed**: While individual results may vary, the combination of pruning and quantization is designed to deliver significant speed improvements on mobile hardware with dedicated INT8 support.\n\n**📊 Accuracy Preservation**: The pipeline maintains accuracy within acceptable thresholds, showing the resilience of MobileNetV3 architecture to compression techniques.\n\n### Stage-by-Stage Technical Analysis\n\n#### Stage 1: L1 Unstructured Pruning (30% sparsity)\n- **Mechanism**: Removes 30% of parameters with smallest L1 magnitude across all Conv2d and Linear layers\n- **Architecture Impact**: Creates sparse weight matrices while preserving original model structure\n- **Key Insight**: File size remains unchanged initially since sparse weights are masked (not removed), but computational complexity is reduced\n\n#### Stage 2: Dynamic Quantization (INT8)\n- **Mechanism**: Converts FP32 weights to INT8 precision, with activations quantized during inference\n- **Backend Optimization**: Uses QNNPACK engine for ARM processor compatibility\n- **Size Impact**: Delivers the primary size reduction by reducing precision from 32-bit to 8-bit representation\n\n### Technical Insights and Optimizations\n\n#### MobileNetV3 Architecture Advantages\n1. **Quantization-Friendly Design**: Hard Swish activations and depthwise separable convolutions are well-suited for quantization\n2. **Pruning Compatibility**: The bottleneck structure with expansion layers provides natural pruning points\n3. **Mobile-First Architecture**: Already optimized for efficiency, making compression more effective\n\n#### Pipeline Sequencing Strategy\n- **Pruning First**: Reduces parameter count before quantization, optimizing the quantization process\n- **Quantization Second**: Applied to the already-pruned model for maximum efficiency\n- **Complementary Effects**: Techniques work synergistically rather than competitively\n\n### Trade-offs and Engineering Decisions\n\n#### Performance vs. Compression Balance\n- **Aggressive Compression**: 30% pruning ratio chosen to achieve significant parameter reduction\n- **Dynamic vs. Static Quantization**: Dynamic chosen for implementation simplicity while still achieving substantial gains\n- **Accuracy Tolerance**: Balanced compression ratio against accuracy preservation requirements\n\n#### Mobile Deployment Considerations\n- **ARM Compatibility**: QNNPACK backend ensures optimal performance on ARM processors\n- **Memory Efficiency**: Reduced model size directly translates to lower memory footprint\n- **Inference Speed**: INT8 operations significantly faster on mobile hardware with dedicated support\n\n### Lessons Learned and Best Practices\n\n#### Compression Technique Ordering\n1. **Sequential Application**: Order matters significantly - pruning before quantization is more effective than the reverse\n2. **Cumulative Effects**: Each technique builds upon the previous optimization\n3. **Compatibility**: Certain techniques work better together than in isolation\n\n#### MobileNetV3-Specific Insights\n- **Robust Architecture**: Shows good resistance to aggressive compression\n- **Efficiency Bottlenecks**: Certain layers more critical than others for performance preservation\n- **Quantization Tolerance**: Architecture handles precision reduction well due to design choices\n\n### Future Improvement Opportunities\n\n#### Advanced Optimization Techniques\n1. **Knowledge Distillation**: Could recover accuracy if needed while maintaining compression gains\n2. **Structured Pruning**: Channel-level pruning for actual architectural simplification\n3. **Neural Architecture Search**: Automated optimization for specific deployment constraints\n\n#### Deployment-Specific Optimizations\n- **Hardware-Aware Quantization**: Calibration data for static quantization on target devices\n- **Framework Optimization**: TensorRT or CoreML conversion for additional hardware acceleration\n- **Edge Computing Integration**: Optimization for specific edge computing platforms\n\n### Recommendations for Production Deployment\n\n#### Immediate Actions\n1. **Mobile Testing**: Validate performance on target mobile devices\n2. **Accuracy Validation**: Extended testing on larger datasets if available\n3. **Model Conversion**: Prepare mobile-specific formats (TorchScript, ONNX, etc.)\n\n#### Long-term Strategy\n- **Continuous Monitoring**: Track performance degradation over time\n- **Iterative Improvement**: Regular retraining and re-optimization cycles\n- **Hardware Evolution**: Adapt to new mobile hardware capabilities\n\n### Conclusion\nThe multi-stage compression pipeline demonstrates that modern neural networks can be significantly optimized for mobile deployment while maintaining acceptable performance characteristics. The combination of pruning and quantization provides a robust foundation for deploying computer vision models in resource-constrained environments, with MobileNetV3's architecture proving particularly well-suited for these optimization techniques."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 🚀 **Next Step:** \n",
    "> Deploy the final model, optimized via the multi-step pipeline, in notebook `04_deployment.ipynb`  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}