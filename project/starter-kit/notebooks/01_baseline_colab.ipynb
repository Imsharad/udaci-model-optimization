{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UdaciSense: Optimized Object Recognition - Google Colab Version\n",
    "\n",
    "## Notebook 1: Baseline Performance\n",
    "\n",
    "In this notebook, you'll establish the baseline performance of the computer vision model. This will serve as the reference point for your optimization efforts.\n",
    "\n",
    "Remember, the CTO has set specific requirements:\n",
    "- The optimized model should be **30% smaller** than the baseline\n",
    "- The optimized model should **reduce inference time by 40%**\n",
    "- The optimized model should **maintain accuracy within 5%** of the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Colab Setup (Clone Repository and Install Dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository to get all files and dependencies\n",
    "!git clone https://github.com/Imsharad/udaci-model-optimization.git\n",
    "%cd udaci-model-optimization/project/starter-kit/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install UV for faster package management\\n!curl -LsSf https://astral.sh/uv/install.sh | sh\\n!source $HOME/.cargo/env\\n\\n# Install PyTorch with CUDA support using UV (much faster)\\n!uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --system"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install additional dependencies with UV (faster installation)\\n!uv pip install matplotlib seaborn pandas scikit-learn pillow tqdm thop plotly tensorboard --system"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "# Import custom modules\n",
    "from utils import MAX_ALLOWED_ACCURACY_DROP, TARGET_INFERENCE_SPEEDUP, TARGET_MODEL_COMPRESSION\n",
    "from utils.data_loader import get_household_loaders, get_input_size, print_dataloader_stats, visualize_batch\n",
    "from utils.model import MobileNetV3_Household, load_model, print_model_summary, train_model\n",
    "from utils.evaluation import calculate_confusion_matrix, evaluate_model_metrics\n",
    "from utils.visualization import plot_confusion_matrix, plot_training_history, plot_weight_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "devices = [\"cpu\"]\n",
    "if torch.cuda.is_available():\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    devices.extend([f\"cuda:{i} ({torch.cuda.get_device_name(i)})\" for i in range(num_devices)])\n",
    "print(f\"Devices available: {devices}\")\n",
    "\n",
    "# Set device to cuda, if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_deterministic_mode(seed):\n",
    "    # Basic seed setting\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # Make cudnn deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # For some PyTorch operations\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "    # For DataLoader workers\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = seed + worker_id\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "    \n",
    "    return seed_worker\n",
    "\n",
    "set_deterministic_mode(42)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "model_type = \"baseline_mobilenet\"\n",
    "models_dir = f\"../models/{model_type}\"\n",
    "models_ckp_dir = f\"{models_dir}/checkpoints\"\n",
    "results_dir = f\"../results/{model_type}\"\n",
    "\n",
    "os.makedirs(models_ckp_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load household objects dataset\n",
    "train_loader, test_loader = get_household_loaders(\n",
    "    image_size=\"CIFAR\", batch_size=128, num_workers=2,\n",
    ")\n",
    "\n",
    "# Get class names\n",
    "class_names = train_loader.dataset.classes\n",
    "print(f\"Datasets have these classes: \")\n",
    "for i in range(len(class_names)):\n",
    "    print(f\"  {i}: {class_names[i]}\")\n",
    "\n",
    "# Visualize some examples\n",
    "for dataset_type, data_loader in [('train', train_loader), ('test', test_loader)]:\n",
    "    print(f\"\\nInformation on {dataset_type} set\")\n",
    "    print_dataloader_stats(data_loader, dataset_type)\n",
    "    print(f\"Examples of images from the {dataset_type} set\")\n",
    "    visualize_batch(data_loader, num_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Train the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = MobileNetV3_Household().to(device)\n",
    "print_model_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training configuration\n",
    "num_epochs = 50\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.001,  # Note that MobileNet is sensitive to high LRs\n",
    "    weight_decay=1e-4,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=0.005,  # Peak learning rate\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=num_epochs,\n",
    "    pct_start=0.3,  # Spend 30% of training time warming up\n",
    "    div_factor=25,  # Initial LR is max_lr/25\n",
    "    final_div_factor=1000  # Final LR is max_lr/1000\n",
    ")\n",
    "\n",
    "training_config = {\n",
    "    'num_epochs': num_epochs,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'scheduler': scheduler,\n",
    "    'patience': 5,\n",
    "    'device': device\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model given the training_config\n",
    "training_stats, best_accuracy, best_epoch = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    training_config,\n",
    "    checkpoint_path=f\"{models_ckp_dir}/model.pth\",\n",
    ")\n",
    "\n",
    "# Save training statistics\n",
    "with open(f\"{results_dir}/training_stats.json\", 'w') as f:\n",
    "    json.dump(training_stats, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Evaluate the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model = load_model(f\"{models_ckp_dir}/model.pth\", device)\n",
    "\n",
    "# Define evaluation input and output variables\n",
    "class_names = test_loader.dataset.classes\n",
    "n_classes = len(class_names)\n",
    "input_size = get_input_size(\"CIFAR\")\n",
    "\n",
    "# Calculate and save model performance on all metrics\n",
    "print(\"Evaluating model's performance on all metrics...\")\n",
    "baseline_metrics = evaluate_model_metrics(model, test_loader, device, n_classes, class_names, input_size, save_path=f\"{results_dir}/metrics.json\")\n",
    "\n",
    "# Calculate, plot, and save confusion matrix\n",
    "confusion_matrix = calculate_confusion_matrix(model, test_loader, device, n_classes)\n",
    "_ = plot_confusion_matrix(confusion_matrix, class_names, f\"{results_dir}/confusion_matrix.png\")\n",
    "\n",
    "# Plot and save training history\n",
    "_ = plot_training_history(training_stats, f\"{results_dir}/training_history.png\")\n",
    "\n",
    "# Plot weight distribution (can help guide optimization strategies)\n",
    "_ = plot_weight_distribution(model, output_path=f\"{results_dir}/weight_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Identify potential optimization approaches\n",
    "Based on our baseline analysis, let's identify promising optimization approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nAll artifacts saved to:\")\n",
    "print(f\" - Model: {models_ckp_dir}/model.pth\")\n",
    "print(f\" - Metrics: {results_dir}/metrics.json\")\n",
    "print(f\" - Confusion Matrix: {results_dir}/confusion_matrix.png\")\n",
    "print(f\" - Training History: {results_dir}/training_history.png\")\n",
    "print(f\" - Training Stats: {results_dir}/training_stats.json\")\n",
    "print(f\" - Weight Distribution: {results_dir}/weight_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate target metrics based on CTO requirements\n",
    "target_model_size = baseline_metrics['size']['model_size_mb'] * (1 - TARGET_MODEL_COMPRESSION)\n",
    "target_inference_time_cpu = baseline_metrics['timing']['cpu']['avg_time_ms'] * (1 - TARGET_INFERENCE_SPEEDUP)\n",
    "if torch.cuda.is_available():\n",
    "    target_inference_time_gpu = baseline_metrics['timing']['cuda']['avg_time_ms'] * (1 - TARGET_INFERENCE_SPEEDUP)\n",
    "min_acceptable_accuracy = baseline_metrics['accuracy']['top1_acc'] * (1 - MAX_ALLOWED_ACCURACY_DROP) \n",
    "\n",
    "print(\"Optimization Targets:\")\n",
    "print(f\"Target Model Size: {baseline_metrics['size']['model_size_mb']:.2f} --> {target_model_size:.2f} MB ({TARGET_MODEL_COMPRESSION*100}% reduction)\")\n",
    "print(f\"Target Inference Time (CPU): {baseline_metrics['timing']['cpu']['avg_time_ms']:.2f} --> {target_inference_time_cpu:.2f} ms ({TARGET_INFERENCE_SPEEDUP*100}% reduction)\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Target Inference Time (GPU): {baseline_metrics['timing']['cuda']['avg_time_ms']:.2f} --> {target_inference_time_gpu:.2f} ms ({TARGET_INFERENCE_SPEEDUP*100}% reduction)\")\n",
    "print(f\"Minimum Acceptable Accuracy: {baseline_metrics['accuracy']['top1_acc']:.2f} --> {min_acceptable_accuracy:.2f} (within {MAX_ALLOWED_ACCURACY_DROP*100}% of baseline)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "**TODO: Analyze the baseline results and select appropriate compression techniques**\n",
    "\n",
    "Now that you've established the baseline performance metrics for the UdaciSense object recognition model, complete an analysis that explores the optimization potential for this specific model architecture.\n",
    "\n",
    "Consider these guiding questions:\n",
    "- What is the baseline performance we need to optimize from?\n",
    "- What characteristics of MobileNetV3 affect its optimization potential?\n",
    "- Which compression techniques are most promising for this architecture and why?\n",
    "- What trade-offs do you anticipate between size, speed, and accuracy?\n",
    "- How might different techniques complement each other in a multi-stage approach?\n",
    "\n",
    "Provide a well-reasoned analysis that includes recommendations for at least two specific compression techniques to implement in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Analysis for UdaciSense Computer Vision Model\n",
    "\n",
    "*(Replace this with your analysis)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸš€ **Next Step:** \n",
    "> Experiment with the compression techniques you've chosen in notebook `02_compression.ipynb`  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}